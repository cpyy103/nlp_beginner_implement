{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1588259578829,
     "user": {
      "displayName": "TM Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghc_jeAWH7aRI3h7J8qfWfGXWbbOCLHF_VHLgw_=s64",
      "userId": "04748394842667443126"
     },
     "user_tz": -480
    },
    "id": "fkQXjmFdhuNx",
    "outputId": "9c9a7a89-ad64-4632-c917-59210d8f35f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3126,
     "status": "ok",
     "timestamp": 1588259581695,
     "user": {
      "displayName": "TM Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghc_jeAWH7aRI3h7J8qfWfGXWbbOCLHF_VHLgw_=s64",
      "userId": "04748394842667443126"
     },
     "user_tz": -480
    },
    "id": "TNfZ_l62iGmg",
    "outputId": "0f3a27a7-97dc-41c8-d3ae-80a3a67b7ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 30 15:12:57 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!/opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXbAOaePht8c"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1115,
     "status": "ok",
     "timestamp": 1588259586292,
     "user": {
      "displayName": "TM Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghc_jeAWH7aRI3h7J8qfWfGXWbbOCLHF_VHLgw_=s64",
      "userId": "04748394842667443126"
     },
     "user_tz": -480
    },
    "id": "4MNiz3y4stPR",
    "outputId": "05089e89-d028-4161-adac-41eddd9dfd77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zRNgQl1s22v"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/content/drive/My Drive/EISM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYJUJKTquk24"
   },
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6p_vtEy_uoCp"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True, batch_first=True, fix_length=50)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train, valid, test = datasets.SNLI.splits(TEXT, LABEL)\n",
    "\n",
    "TEXT.build_vocab(train, valid, test, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiQ3qSsav19k"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "    datasets=(train, valid, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXUIkyokukuQ"
   },
   "source": [
    "## 相关配置参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24758,
     "status": "ok",
     "timestamp": 1588259616412,
     "user": {
      "displayName": "TM Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghc_jeAWH7aRI3h7J8qfWfGXWbbOCLHF_VHLgw_=s64",
      "userId": "04748394842667443126"
     },
     "user_tz": -480
    },
    "id": "V-8vd4kHtRlA",
    "outputId": "77de7066-93be-44df-c0bd-7d879c6864b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57324, 100])\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.batch_size = batch_size\n",
    "        # embedding\n",
    "        print(TEXT.vocab.vectors.size())\n",
    "        self.vocab_size = TEXT.vocab.vectors.size()[0]\n",
    "        self.embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "        # lstm\n",
    "        self.hidden_dim = 200\n",
    "        self.num_layers = 1\n",
    "        # fc\n",
    "        self.linear_size = 200\n",
    "        self.dropout = 0.3\n",
    "        self.output_dim = len(LABEL.vocab)\n",
    "\n",
    "        # train\n",
    "        self.learning_rate = 1e-3\n",
    "        self.epochs = 5\n",
    "\n",
    "        # model\n",
    "        self.model_path = '.model.pkl'\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JXU8VQRywrv"
   },
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ESIM, self).__init__()\n",
    "        self.args = args\n",
    "        self.embedding = nn.Embedding(self.args.vocab_size, self.args.embedding_dim).from_pretrained(TEXT.vocab.vectors)\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            self.args.embedding_dim,\n",
    "            self.args.hidden_dim, \n",
    "            num_layers=self.args.num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            self.args.hidden_dim*8, \n",
    "            self.args.hidden_dim, \n",
    "            num_layers=self.args.num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.args.hidden_dim*8),\n",
    "            nn.Linear(self.args.hidden_dim*8, self.args.linear_size),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(self.args.linear_size),\n",
    "            nn.Dropout(self.args.dropout),\n",
    "            nn.Linear(self.args.linear_size, self.args.linear_size),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(self.args.linear_size),\n",
    "            nn.Dropout(self.args.dropout),\n",
    "            nn.Linear(self.args.linear_size, self.args.output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "            )\n",
    "        \n",
    "    def submul(self, x1, x2):\n",
    "        sub = x1 - x2\n",
    "        mul = x1 * x2\n",
    "        return torch.cat([sub, mul], dim=-1)\n",
    "    \n",
    "    def apply_multiple(self, x):\n",
    "        # input [batch_size, sequence_length, 2*hidden_dim]\n",
    "        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\n",
    "        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)\n",
    "        # output [batch_size, 4*hidden_dim]\n",
    "        return torch.cat([p1, p2], dim=1)\n",
    "    \n",
    "    def soft_attention_align(self, x1, x2, mask1, mask2):\n",
    "        # x1 x2 [batch_size, sequence_length, 2*hidden_dim]\n",
    "\n",
    "        # attention [batch_size, sequence_length, sequence_length]\n",
    "        attention = torch.matmul(x1, x2.transpose(1, 2))\n",
    "       \n",
    "        # 放置softmax时出现异常值\n",
    "        mask1 = mask1.float().masked_fill_(mask1, float('-inf'))\n",
    "        mask2 = mask2.float().masked_fill_(mask2, float('-inf'))\n",
    "\n",
    "        # weight [batch_size, sequence_length, sequence_length]\n",
    "        # x_align [batch_size, sequence_length, 2*hidden_dim]\n",
    "        weight1 = F.softmax(attention + mask2.unsqueeze(1), dim=-1)\n",
    "        x1_align = torch.matmul(weight1, x2)\n",
    "        weight2 = F.softmax(attention.transpose(1,2) + mask1.unsqueeze(1), dim=-1)\n",
    "        x2_align = torch.matmul(weight2, x1)\n",
    "\n",
    "        return x1_align, x2_align\n",
    "    \n",
    "    def forward(self, sequence1, sequence2):\n",
    "        # input sequence [batch_size, sequence_length]\n",
    "\n",
    "        # x [batch_size, sequence_length, embedding_dim]\n",
    "        x1, x2 = self.embedding(sequence1), self.embedding(sequence2)\n",
    "\n",
    "        mask1, mask2 = sequence1.eq(0), sequence2.eq(0)\n",
    "        # print(mask1)\n",
    "        # print(mask2)\n",
    "        # out1 out2 [batch_size, sequence_length, 2*hidden_dim]\n",
    "        out1, _ = self.lstm1(x1)\n",
    "        out2, _ = self.lstm1(x2)\n",
    "\n",
    "        # x_align [batch_size, sequence_length, 2*hidden_dim]\n",
    "        x1_align, x2_align = self.soft_attention_align(out1, out2, mask1, mask2)\n",
    "    \n",
    "        # x1 x2 [batch_size, sequence_length, 8*hidden_dim]\n",
    "        x1 = torch.cat([out1, x1_align, self.submul(out1, x1_align)], dim=-1)\n",
    "        x2 = torch.cat([out2, x2_align, self.submul(out2, x2_align)], dim=-1)\n",
    "\n",
    "        # out1 out2 [batch_size, sequence_length, hidden_dim]\n",
    "        out1, _ = self.lstm2(x1)\n",
    "        out2, _ = self.lstm2(x2)\n",
    "        \n",
    "        # x1 x2 [batch_size, 4*hidden_dim]\n",
    "        x1 = self.apply_multiple(out1)\n",
    "        x2 = self.apply_multiple(out2)\n",
    "\n",
    "        # out [batch_szie, num_classes]\n",
    "        out = self.fc(torch.cat([x1, x2], dim=-1))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10064,
     "status": "ok",
     "timestamp": 1588259616413,
     "user": {
      "displayName": "TM Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghc_jeAWH7aRI3h7J8qfWfGXWbbOCLHF_VHLgw_=s64",
      "userId": "04748394842667443126"
     },
     "user_tz": -480
    },
    "id": "YOjMFEyJywoh",
    "outputId": "d0fd77cb-ef3d-412b-d7d3-f1e1d6512d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESIM(\n",
       "  (embedding): Embedding(57324, 100)\n",
       "  (lstm1): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
       "  (lstm2): LSTM(1600, 200, batch_first=True, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): BatchNorm1d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=1600, out_features=200, bias=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (6): ELU(alpha=1.0, inplace=True)\n",
       "    (7): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=200, out_features=4, bias=True)\n",
       "    (10): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ESIM(args)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsMtdvNf8rP"
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNijlLCtywlu"
   },
   "outputs": [],
   "source": [
    "def train(model, data_iter, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_train_num = len(data_iter.dataset)\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        x1 = batch.premise.to(device)\n",
    "        x2 = batch.hypothesis.to(device)\n",
    "        label = batch.label.to(device)\n",
    "\n",
    "        y_pred = model(x1, x2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_accuracy = (torch.argmax(y_pred, dim=1)==label).sum().item()\n",
    "        total_accuracy += batch_accuracy\n",
    "\n",
    "        if i%200==0:\n",
    "            print('Batch_{}, Train Loss:{}, Accuracy:{}'.format(i, batch_loss/len(label), batch_accuracy/len(label)))\n",
    "\n",
    "    return total_loss/total_train_num, total_accuracy/total_train_num\n",
    "        \n",
    "\n",
    "def valid(model, data_iter, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_valid_num = len(data_iter.dataset)\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        x1 = batch.premise.to(device)\n",
    "        x2 = batch.hypothesis.to(device)\n",
    "        label = batch.label.to(device)\n",
    "\n",
    "        y_pred = model(x1, x2)\n",
    "\n",
    "        loss = loss_fn(y_pred, label)\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_accuracy = (torch.argmax(y_pred, dim=1)==label).sum().item()\n",
    "        total_accuracy += batch_accuracy\n",
    "\n",
    "    return total_loss/total_valid_num, total_accuracy/total_valid_num\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4549181,
     "status": "ok",
     "timestamp": 1588264177520,
     "user": {
      "displayName": "TM Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghc_jeAWH7aRI3h7J8qfWfGXWbbOCLHF_VHLgw_=s64",
      "userId": "04748394842667443126"
     },
     "user_tz": -480
    },
    "id": "VOuJLKUyywgQ",
    "outputId": "bc184fe5-9276-4dc0-a9e0-e7912223096d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch_0---------------\n",
      "Batch_0, Train Loss:0.0109553849324584, Accuracy:0.2421875\n",
      "Batch_200, Train Loss:0.009341755881905556, Accuracy:0.515625\n",
      "Batch_400, Train Loss:0.00895842257887125, Accuracy:0.578125\n",
      "Batch_600, Train Loss:0.008931586518883705, Accuracy:0.6171875\n",
      "Batch_800, Train Loss:0.00866751279681921, Accuracy:0.6328125\n",
      "Batch_1000, Train Loss:0.008087384514510632, Accuracy:0.703125\n",
      "Batch_1200, Train Loss:0.008174305781722069, Accuracy:0.6796875\n",
      "Batch_1400, Train Loss:0.00828133150935173, Accuracy:0.671875\n",
      "Batch_1600, Train Loss:0.008321967907249928, Accuracy:0.671875\n",
      "Batch_1800, Train Loss:0.008685889653861523, Accuracy:0.625\n",
      "Batch_2000, Train Loss:0.008673866279423237, Accuracy:0.6171875\n",
      "Batch_2200, Train Loss:0.008319648914039135, Accuracy:0.6796875\n",
      "Batch_2400, Train Loss:0.008317834697663784, Accuracy:0.6640625\n",
      "Batch_2600, Train Loss:0.007890625856816769, Accuracy:0.734375\n",
      "Batch_2800, Train Loss:0.008319636806845665, Accuracy:0.6796875\n",
      "Batch_3000, Train Loss:0.008346968330442905, Accuracy:0.6640625\n",
      "Batch_3200, Train Loss:0.008328904397785664, Accuracy:0.6953125\n",
      "Batch_3400, Train Loss:0.008323116227984428, Accuracy:0.6796875\n",
      "Batch_3600, Train Loss:0.008322599343955517, Accuracy:0.6796875\n",
      "Batch_3800, Train Loss:0.00827518105506897, Accuracy:0.6875\n",
      "Batch_4000, Train Loss:0.007714624051004648, Accuracy:0.75\n",
      "Batch_4200, Train Loss:0.008479614742100239, Accuracy:0.6328125\n",
      ">>>Epoch_0\n",
      ">>>Train Loss:0.008413178686257898, Accuracy:0.6581756821942345\n",
      ">>>Valid Loss:0.008004010147593948, Accuracy:0.713777687461898\n",
      "\n",
      "---------------Epoch_1---------------\n",
      "Batch_0, Train Loss:0.008045998401939869, Accuracy:0.703125\n",
      "Batch_200, Train Loss:0.00834917277097702, Accuracy:0.6640625\n",
      "Batch_400, Train Loss:0.007931683212518692, Accuracy:0.71875\n",
      "Batch_600, Train Loss:0.008184429258108139, Accuracy:0.6796875\n",
      "Batch_800, Train Loss:0.008103206753730774, Accuracy:0.6953125\n",
      "Batch_1000, Train Loss:0.007882512174546719, Accuracy:0.734375\n",
      "Batch_1200, Train Loss:0.007739974185824394, Accuracy:0.734375\n",
      "Batch_1400, Train Loss:0.00819383841007948, Accuracy:0.6796875\n",
      "Batch_1600, Train Loss:0.008160742931067944, Accuracy:0.703125\n",
      "Batch_1800, Train Loss:0.007964048534631729, Accuracy:0.71875\n",
      "Batch_2000, Train Loss:0.008450902067124844, Accuracy:0.65625\n",
      "Batch_2200, Train Loss:0.007633659988641739, Accuracy:0.765625\n",
      "Batch_2400, Train Loss:0.007710529025644064, Accuracy:0.7578125\n",
      "Batch_2600, Train Loss:0.008599977940320969, Accuracy:0.625\n",
      "Batch_2800, Train Loss:0.008435883559286594, Accuracy:0.65625\n",
      "Batch_3000, Train Loss:0.008110241033136845, Accuracy:0.71875\n",
      "Batch_3200, Train Loss:0.00808104407042265, Accuracy:0.703125\n",
      "Batch_3400, Train Loss:0.008448698557913303, Accuracy:0.65625\n",
      "Batch_3600, Train Loss:0.008069101721048355, Accuracy:0.6953125\n",
      "Batch_3800, Train Loss:0.008007784374058247, Accuracy:0.703125\n",
      "Batch_4000, Train Loss:0.008143608458340168, Accuracy:0.6796875\n",
      "Batch_4200, Train Loss:0.008079354651272297, Accuracy:0.703125\n",
      ">>>Epoch_1\n",
      ">>>Train Loss:0.008004578560377602, Accuracy:0.712986036656734\n",
      ">>>Valid Loss:0.007876870236583804, Accuracy:0.7289168868116237\n",
      "\n",
      "---------------Epoch_2---------------\n",
      "Batch_0, Train Loss:0.007627775892615318, Accuracy:0.7578125\n",
      "Batch_200, Train Loss:0.007776801008731127, Accuracy:0.734375\n",
      "Batch_400, Train Loss:0.007745086681097746, Accuracy:0.75\n",
      "Batch_600, Train Loss:0.007361982949078083, Accuracy:0.796875\n",
      "Batch_800, Train Loss:0.007825533859431744, Accuracy:0.7421875\n",
      "Batch_1000, Train Loss:0.008065796457231045, Accuracy:0.7109375\n",
      "Batch_1200, Train Loss:0.007414279505610466, Accuracy:0.78125\n",
      "Batch_1400, Train Loss:0.007696008775383234, Accuracy:0.765625\n",
      "Batch_1600, Train Loss:0.007659673225134611, Accuracy:0.765625\n",
      "Batch_1800, Train Loss:0.007579760160297155, Accuracy:0.78125\n",
      "Batch_2000, Train Loss:0.007940062321722507, Accuracy:0.71875\n",
      "Batch_2200, Train Loss:0.008026794530451298, Accuracy:0.7265625\n",
      "Batch_2400, Train Loss:0.007867695763707161, Accuracy:0.7109375\n",
      "Batch_2600, Train Loss:0.007661853916943073, Accuracy:0.7578125\n",
      "Batch_2800, Train Loss:0.0076978253200650215, Accuracy:0.7421875\n",
      "Batch_3000, Train Loss:0.007570600137114525, Accuracy:0.7734375\n",
      "Batch_3200, Train Loss:0.008403097279369831, Accuracy:0.65625\n",
      "Batch_3400, Train Loss:0.007590117864310741, Accuracy:0.78125\n",
      "Batch_3600, Train Loss:0.007531262002885342, Accuracy:0.7734375\n",
      "Batch_3800, Train Loss:0.007891233079135418, Accuracy:0.7265625\n",
      "Batch_4000, Train Loss:0.007469772361218929, Accuracy:0.796875\n",
      "Batch_4200, Train Loss:0.007604819256812334, Accuracy:0.7421875\n",
      ">>>Epoch_2\n",
      ">>>Train Loss:0.007868760505840885, Accuracy:0.7311378368194668\n",
      ">>>Valid Loss:0.007825396956527312, Accuracy:0.7379597642755538\n",
      "\n",
      "---------------Epoch_3---------------\n",
      "Batch_0, Train Loss:0.007098053582012653, Accuracy:0.828125\n",
      "Batch_200, Train Loss:0.00794253870844841, Accuracy:0.7109375\n",
      "Batch_400, Train Loss:0.007545390632003546, Accuracy:0.765625\n",
      "Batch_600, Train Loss:0.0074422117322683334, Accuracy:0.796875\n",
      "Batch_800, Train Loss:0.008104224689304829, Accuracy:0.7109375\n",
      "Batch_1000, Train Loss:0.008371287025511265, Accuracy:0.640625\n",
      "Batch_1200, Train Loss:0.0075165703892707825, Accuracy:0.7734375\n",
      "Batch_1400, Train Loss:0.008114604279398918, Accuracy:0.6953125\n",
      "Batch_1600, Train Loss:0.007788344752043486, Accuracy:0.7421875\n",
      "Batch_1800, Train Loss:0.008152212016284466, Accuracy:0.6875\n",
      "Batch_2000, Train Loss:0.008179805241525173, Accuracy:0.6875\n",
      "Batch_2200, Train Loss:0.007750239688903093, Accuracy:0.7421875\n",
      "Batch_2400, Train Loss:0.007712048478424549, Accuracy:0.7578125\n",
      "Batch_2600, Train Loss:0.007529163267463446, Accuracy:0.78125\n",
      "Batch_2800, Train Loss:0.007709536701440811, Accuracy:0.75\n",
      "Batch_3000, Train Loss:0.007290228735655546, Accuracy:0.8125\n",
      "Batch_3200, Train Loss:0.00791827030479908, Accuracy:0.703125\n",
      "Batch_3400, Train Loss:0.008324552327394485, Accuracy:0.6484375\n",
      "Batch_3600, Train Loss:0.007958194240927696, Accuracy:0.71875\n",
      "Batch_3800, Train Loss:0.007601655554026365, Accuracy:0.78125\n",
      "Batch_4000, Train Loss:0.0076974425464868546, Accuracy:0.75\n",
      "Batch_4200, Train Loss:0.007465539965778589, Accuracy:0.796875\n",
      ">>>Epoch_3\n",
      ">>>Train Loss:0.007776258894233695, Accuracy:0.7431097972757738\n",
      ">>>Valid Loss:0.007751108966109182, Accuracy:0.7480186953871164\n",
      "\n",
      "---------------Epoch_4---------------\n",
      "Batch_0, Train Loss:0.007007450331002474, Accuracy:0.84375\n",
      "Batch_200, Train Loss:0.007379637565463781, Accuracy:0.7890625\n",
      "Batch_400, Train Loss:0.0078107621520757675, Accuracy:0.734375\n",
      "Batch_600, Train Loss:0.007727176882326603, Accuracy:0.7578125\n",
      "Batch_800, Train Loss:0.007905546575784683, Accuracy:0.7421875\n",
      "Batch_1000, Train Loss:0.007855053059756756, Accuracy:0.734375\n",
      "Batch_1200, Train Loss:0.008242526091635227, Accuracy:0.671875\n",
      "Batch_1400, Train Loss:0.007908649742603302, Accuracy:0.7109375\n",
      "Batch_1600, Train Loss:0.007120998110622168, Accuracy:0.828125\n",
      "Batch_1800, Train Loss:0.007214637007564306, Accuracy:0.8203125\n",
      "Batch_2000, Train Loss:0.007466157432645559, Accuracy:0.7890625\n",
      "Batch_2200, Train Loss:0.007866737432777882, Accuracy:0.734375\n",
      "Batch_2400, Train Loss:0.007500903215259314, Accuracy:0.78125\n",
      "Batch_2600, Train Loss:0.008266622200608253, Accuracy:0.6796875\n",
      "Batch_2800, Train Loss:0.007629715837538242, Accuracy:0.7578125\n",
      "Batch_3000, Train Loss:0.00777408154681325, Accuracy:0.7265625\n",
      "Batch_3200, Train Loss:0.0082930326461792, Accuracy:0.6875\n",
      "Batch_3400, Train Loss:0.007441450841724873, Accuracy:0.78125\n",
      "Batch_3600, Train Loss:0.0073845162987709045, Accuracy:0.8046875\n",
      "Batch_3800, Train Loss:0.00784147996455431, Accuracy:0.71875\n",
      "Batch_4000, Train Loss:0.007869033142924309, Accuracy:0.7265625\n",
      "Batch_4200, Train Loss:0.008044424466788769, Accuracy:0.6953125\n",
      ">>>Epoch_4\n",
      ">>>Train Loss:0.007699246389885704, Accuracy:0.7536455593437538\n",
      ">>>Valid Loss:0.007732157741371051, Accuracy:0.7496443812233285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ESIM(args).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(args.epochs):\n",
    "    print('---------------Epoch_{}---------------'.format(e))\n",
    "    train_loss, train_accuracy = train(model, train_iter, loss_fn, optimizer)\n",
    "    valid_loss, valid_accuracy = valid(model, valid_iter, loss_fn)\n",
    "    print('>>>Epoch_{}\\n>>>Train Loss:{}, Accuracy:{}'.format(e, train_loss, train_accuracy))\n",
    "    print('>>>Valid Loss:{}, Accuracy:{}\\n'.format(valid_loss, valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyMQiaMni8wa"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTEffrrkkZW3"
   },
   "outputs": [],
   "source": [
    "model = ESIM(args)\n",
    "model.load_state_dict(torch.load(args.model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uySsIEcakhLF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CLV2RtHkhIQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuLrAftfkhFs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOfxb+bfula4EZ8r8f5Mzt5",
   "collapsed_sections": [],
   "mount_file_id": "1ZQ_aSw2QXaSNFivGShceg2i8lJY23J0g",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
