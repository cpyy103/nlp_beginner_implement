{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于深度学习的文本分类 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./input/train.tsv', sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(train_df))\n",
    "np.random.shuffle(index)\n",
    "\n",
    "train_size = 0.8\n",
    "train_num = int(len(index)*train_size)\n",
    "\n",
    "train_df.iloc[index[:train_num], :].to_csv('./input/task2_train.csv', index=False)\n",
    "train_df.iloc[index[train_num:], :].to_csv('./input/task2_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True, fix_length=100)\n",
    "LABEL = data.Field(sequential=False, batch_first=True, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('PhraseId', None),\n",
    "              ('SentenceId', None),\n",
    "              ('Phrase', TEXT),\n",
    "              ('Sentiment', LABEL)]\n",
    "\n",
    "train_data = data.TabularDataset(path='./input/task2_train.csv', format='csv', skip_header=True, fields=data_fields)\n",
    "valid_data = data.TabularDataset(path='./input/task2_valid.csv', format='csv', skip_header=True, fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_iter = data.Iterator(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_iter = data.Iterator(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embedding = len(TEXT.vocab)\n",
    "embedding_dim = 100 # 根据使用的glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.channel_num = 16\n",
    "        self.class_num = 5\n",
    "        self.dropout = 0.1\n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim).from_pretrained(TEXT.vocab.vectors)\n",
    "        self.conv1 = nn.Conv2d(1, self.channel_num, kernel_size=(3, embedding_dim), padding=(2, 0))\n",
    "        self.conv2 = nn.Conv2d(1, self.channel_num, kernel_size=(4, embedding_dim), padding=(3, 0))\n",
    "        self.conv3 = nn.Conv2d(1, self.channel_num, kernel_size=(5, embedding_dim), padding=(4, 0))\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.fc = nn.Linear(3*self.channel_num, self.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x [batch_size, sequence_length]\n",
    "        x = self.embedding(x)\n",
    "        # [batch_size, sequence_length, embedding_dim]\n",
    "        x = x.unsqueeze(1)\n",
    "        # [batch_size, 1, sequence_length, embedding_dim]\n",
    "        x1 = self.conv_and_pool(x, self.conv1)\n",
    "        x2 = self.conv_and_pool(x, self.conv2)\n",
    "        x3 = self.conv_and_pool(x, self.conv3)\n",
    "        # [batch_size, channel_num]\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        # [batch_size, channel_num*3]\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc(x)\n",
    "        # [batch_size, class_num]\n",
    "        return out\n",
    "        \n",
    "    @staticmethod \n",
    "    def conv_and_pool(x, conv):\n",
    "        # x [batch_size, 1, sequence_length, embedding_dim]\n",
    "        x = conv(x)\n",
    "        # [batch_size, channel_num, H_out, W_out]  \n",
    "        # H_out = (sequence_length+2*padding[0]-(kernel_size[0]-1)-1)/stride[0] + 1\n",
    "        # W_out = (embedding_dim+2*padding[1]-(kernel_size[1]-1)-1)/stride[1] + 1 = 1\n",
    "        x = F.relu(x.squeeze(3))\n",
    "        # [batch_size, channel_num, H_out]\n",
    "        x = F.max_pool1d(x, kernel_size=x.size(2))\n",
    "        # [batch_size, channel_num, L_out]\n",
    "        # L_out = (H_out+2*padding-(kernel_size-1)-1) / stride + 1 = 1\n",
    "        x = x.squeeze(2)\n",
    "        # [batch_size, channel_num]\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_0 Batch_0, Train Loss:0.04957478493452072, Accuracy:0.28125\n",
      "Epoch_0 Batch_200, Train Loss:0.025076357647776604, Accuracy:0.6875\n",
      "Epoch_0 Batch_400, Train Loss:0.040925655514001846, Accuracy:0.34375\n",
      "Epoch_0 Batch_600, Train Loss:0.03395047411322594, Accuracy:0.5625\n",
      "Epoch_0 Batch_800, Train Loss:0.03413496911525726, Accuracy:0.5625\n",
      "Epoch_0 Batch_1000, Train Loss:0.029053203761577606, Accuracy:0.625\n",
      "Epoch_0 Batch_1200, Train Loss:0.02809777483344078, Accuracy:0.71875\n",
      "Epoch_0 Batch_1400, Train Loss:0.02457335963845253, Accuracy:0.65625\n",
      "Epoch_0 Batch_1600, Train Loss:0.027460245415568352, Accuracy:0.625\n",
      "Epoch_0 Batch_1800, Train Loss:0.024487830698490143, Accuracy:0.65625\n",
      "Epoch_0 Batch_2000, Train Loss:0.024921800941228867, Accuracy:0.75\n",
      "Epoch_0 Batch_2200, Train Loss:0.041361577808856964, Accuracy:0.4375\n",
      "Epoch_0 Batch_2400, Train Loss:0.02737363614141941, Accuracy:0.6875\n",
      "Epoch_0 Batch_2600, Train Loss:0.03153635561466217, Accuracy:0.65625\n",
      "Epoch_0 Batch_2800, Train Loss:0.0266115739941597, Accuracy:0.65625\n",
      "Epoch_0 Batch_3000, Train Loss:0.03453110530972481, Accuracy:0.5\n",
      "Epoch_0 Batch_3200, Train Loss:0.036464545875787735, Accuracy:0.5\n",
      "Epoch_0 Batch_3400, Train Loss:0.027829302474856377, Accuracy:0.5625\n",
      "Epoch_0 Batch_3600, Train Loss:0.024805746972560883, Accuracy:0.625\n",
      "Epoch_0 Batch_3800, Train Loss:0.029056474566459656, Accuracy:0.59375\n",
      ">>> Epoch_0, Train Loss:0.031262845542378785, Accuracy:0.5930331282839933\n",
      ">>> Epoch_0, Valid loss:0.02904962466899117, Accuracy:0.6184159938485198 \n",
      "\n",
      "Epoch_1 Batch_0, Train Loss:0.03387520834803581, Accuracy:0.5625\n",
      "Epoch_1 Batch_200, Train Loss:0.03171370178461075, Accuracy:0.46875\n",
      "Epoch_1 Batch_400, Train Loss:0.027773410081863403, Accuracy:0.6875\n",
      "Epoch_1 Batch_600, Train Loss:0.025816071778535843, Accuracy:0.65625\n",
      "Epoch_1 Batch_800, Train Loss:0.024130597710609436, Accuracy:0.59375\n",
      "Epoch_1 Batch_1000, Train Loss:0.03494837135076523, Accuracy:0.53125\n",
      "Epoch_1 Batch_1200, Train Loss:0.02218903973698616, Accuracy:0.6875\n",
      "Epoch_1 Batch_1400, Train Loss:0.02390873432159424, Accuracy:0.71875\n",
      "Epoch_1 Batch_1600, Train Loss:0.0307165440171957, Accuracy:0.625\n",
      "Epoch_1 Batch_1800, Train Loss:0.03428780660033226, Accuracy:0.59375\n",
      "Epoch_1 Batch_2000, Train Loss:0.030217668041586876, Accuracy:0.65625\n",
      "Epoch_1 Batch_2200, Train Loss:0.02140919119119644, Accuracy:0.78125\n",
      "Epoch_1 Batch_2400, Train Loss:0.017771095037460327, Accuracy:0.84375\n",
      "Epoch_1 Batch_2600, Train Loss:0.031158853322267532, Accuracy:0.59375\n",
      "Epoch_1 Batch_2800, Train Loss:0.03294915705919266, Accuracy:0.5625\n",
      "Epoch_1 Batch_3000, Train Loss:0.024281363934278488, Accuracy:0.6875\n",
      "Epoch_1 Batch_3200, Train Loss:0.028600620105862617, Accuracy:0.625\n",
      "Epoch_1 Batch_3400, Train Loss:0.02084609493613243, Accuracy:0.6875\n",
      "Epoch_1 Batch_3600, Train Loss:0.024319767951965332, Accuracy:0.65625\n",
      "Epoch_1 Batch_3800, Train Loss:0.027807924896478653, Accuracy:0.71875\n",
      ">>> Epoch_1, Train Loss:0.028978633611638105, Accuracy:0.6188325003203896\n",
      ">>> Epoch_1, Valid loss:0.028508429997138117, Accuracy:0.6240548506984493 \n",
      "\n",
      "Epoch_2 Batch_0, Train Loss:0.029147159308195114, Accuracy:0.625\n",
      "Epoch_2 Batch_200, Train Loss:0.030020862817764282, Accuracy:0.53125\n",
      "Epoch_2 Batch_400, Train Loss:0.037515658885240555, Accuracy:0.46875\n",
      "Epoch_2 Batch_600, Train Loss:0.026496075093746185, Accuracy:0.6875\n",
      "Epoch_2 Batch_800, Train Loss:0.023532425984740257, Accuracy:0.65625\n",
      "Epoch_2 Batch_1000, Train Loss:0.024920877069234848, Accuracy:0.75\n",
      "Epoch_2 Batch_1200, Train Loss:0.02418597787618637, Accuracy:0.65625\n",
      "Epoch_2 Batch_1400, Train Loss:0.03572145849466324, Accuracy:0.5625\n",
      "Epoch_2 Batch_1600, Train Loss:0.03692755475640297, Accuracy:0.5625\n",
      "Epoch_2 Batch_1800, Train Loss:0.03116757422685623, Accuracy:0.65625\n",
      "Epoch_2 Batch_2000, Train Loss:0.025408120825886726, Accuracy:0.6875\n",
      "Epoch_2 Batch_2200, Train Loss:0.028511250391602516, Accuracy:0.625\n",
      "Epoch_2 Batch_2400, Train Loss:0.02601822279393673, Accuracy:0.6875\n",
      "Epoch_2 Batch_2600, Train Loss:0.02354602701961994, Accuracy:0.65625\n",
      "Epoch_2 Batch_2800, Train Loss:0.02139873243868351, Accuracy:0.65625\n",
      "Epoch_2 Batch_3000, Train Loss:0.03147812932729721, Accuracy:0.625\n",
      "Epoch_2 Batch_3200, Train Loss:0.032762859016656876, Accuracy:0.59375\n",
      "Epoch_2 Batch_3400, Train Loss:0.021655485033988953, Accuracy:0.75\n",
      "Epoch_2 Batch_3600, Train Loss:0.01663966476917267, Accuracy:0.84375\n",
      "Epoch_2 Batch_3800, Train Loss:0.02345285564661026, Accuracy:0.5625\n",
      ">>> Epoch_2, Train Loss:0.028147221391434076, Accuracy:0.6293172497757273\n",
      ">>> Epoch_2, Valid loss:0.027996791227573037, Accuracy:0.6347879020889402 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "for e in range(epoch):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0\n",
    "    total_train_num = len(train_iter.dataset)\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        text = batch.Phrase.to(device)\n",
    "        label = batch.Sentiment.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out =model(text)\n",
    "        loss = criterion(out, label) \n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_accuracy = (torch.argmax(out, dim=1)==label).sum().item()\n",
    "        accuracy += batch_accuracy\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 200 == 0:\n",
    "            print('Epoch_{} Batch_{}, Train Loss:{}, Accuracy:{}'.format(e, i, batch_loss/len(label), batch_accuracy/len(label)))\n",
    "    print('>>> Epoch_{}, Train Loss:{}, Accuracy:{}'.format(e, total_loss/total_train_num, accuracy/total_train_num))\n",
    "    \n",
    "    # 每训练完一个epoch看下\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0\n",
    "    total_valid_num = len(valid_iter.dataset)\n",
    "    for batch in valid_iter:\n",
    "        text = batch.Phrase.to(device)\n",
    "        label = batch.Sentiment.to(device)\n",
    "        \n",
    "        out = model(text)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        total_loss += loss.item()\n",
    "        accuracy += (torch.argmax(out, dim=1)==label).sum().item()\n",
    "        \n",
    "    print('>>> Epoch_{}, Valid loss:{}, Accuracy:{} \\n'.format(e, total_loss/total_valid_num, accuracy/total_valid_num))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上去效果不是很理想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = 55\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.1\n",
    "        self.bidirectional = True\n",
    "        self.class_num = 5\n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim).from_pretrained(TEXT.vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size=self.hidden_size, num_layers = self.num_layers,\n",
    "                           batch_first=True, dropout=self.dropout, bidirectional=self.bidirectional)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.fc = nn.Linear(self.hidden_size*(2 if self.bidirectional else 1), self.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x [batch_size, squence_length]\n",
    "        h0 = torch.zeros(self.num_layers*(2 if self.bidirectional else 1), x.shape[0], self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*(2 if self.bidirectional else 1), x.shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # [batch_size, squence_length, embedding_dim]\n",
    "        \n",
    "        out, hidden = self.lstm(x, (h0, c0))\n",
    "        # out [batch_size, squence_length, hidden_size*2 (bidirectional)]\n",
    "        # hidden  (h1, c1)\n",
    "\n",
    "        out =self.dropout(out)\n",
    "        \n",
    "        out = self.fc(out[:,-1,:]) # 取最后一个输出\n",
    "        \n",
    "        return out \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_0 Batch_0, Train Loss:0.048784174025058746, Accuracy:0.625\n",
      "Epoch_0 Batch_200, Train Loss:0.040376611053943634, Accuracy:0.53125\n",
      "Epoch_0 Batch_400, Train Loss:0.04513956978917122, Accuracy:0.375\n",
      "Epoch_0 Batch_600, Train Loss:0.036570582538843155, Accuracy:0.5\n",
      "Epoch_0 Batch_800, Train Loss:0.03312668576836586, Accuracy:0.59375\n",
      "Epoch_0 Batch_1000, Train Loss:0.046475473791360855, Accuracy:0.34375\n",
      "Epoch_0 Batch_1200, Train Loss:0.04132869467139244, Accuracy:0.53125\n",
      "Epoch_0 Batch_1400, Train Loss:0.03976373001933098, Accuracy:0.53125\n",
      "Epoch_0 Batch_1600, Train Loss:0.04671654850244522, Accuracy:0.34375\n",
      "Epoch_0 Batch_1800, Train Loss:0.035003092139959335, Accuracy:0.6875\n",
      "Epoch_0 Batch_2000, Train Loss:0.038686737418174744, Accuracy:0.59375\n",
      "Epoch_0 Batch_2200, Train Loss:0.04222581908106804, Accuracy:0.5625\n",
      "Epoch_0 Batch_2400, Train Loss:0.04316255822777748, Accuracy:0.46875\n",
      "Epoch_0 Batch_2600, Train Loss:0.03934739902615547, Accuracy:0.4375\n",
      "Epoch_0 Batch_2800, Train Loss:0.03902953863143921, Accuracy:0.53125\n",
      "Epoch_0 Batch_3000, Train Loss:0.04264431819319725, Accuracy:0.40625\n",
      "Epoch_0 Batch_3200, Train Loss:0.038221631199121475, Accuracy:0.59375\n",
      "Epoch_0 Batch_3400, Train Loss:0.03728363290429115, Accuracy:0.59375\n",
      "Epoch_0 Batch_3600, Train Loss:0.034529298543930054, Accuracy:0.71875\n",
      "Epoch_0 Batch_3800, Train Loss:0.03340834006667137, Accuracy:0.625\n",
      ">>> Epoch_0, Train Loss:0.04023931852400906, Accuracy:0.5096917852108164\n",
      ">>> Epoch_0, Valid loss:0.04012195360167094, Accuracy:0.510957324106113 \n",
      "\n",
      "Epoch_1 Batch_0, Train Loss:0.036001842468976974, Accuracy:0.5\n",
      "Epoch_1 Batch_200, Train Loss:0.030272256582975388, Accuracy:0.6875\n",
      "Epoch_1 Batch_400, Train Loss:0.034742143005132675, Accuracy:0.71875\n",
      "Epoch_1 Batch_600, Train Loss:0.036140263080596924, Accuracy:0.5625\n",
      "Epoch_1 Batch_800, Train Loss:0.04328235983848572, Accuracy:0.46875\n",
      "Epoch_1 Batch_1000, Train Loss:0.04160858318209648, Accuracy:0.53125\n",
      "Epoch_1 Batch_1200, Train Loss:0.047613777220249176, Accuracy:0.34375\n",
      "Epoch_1 Batch_1400, Train Loss:0.04511012136936188, Accuracy:0.375\n",
      "Epoch_1 Batch_1600, Train Loss:0.04284823313355446, Accuracy:0.4375\n",
      "Epoch_1 Batch_1800, Train Loss:0.03580135479569435, Accuracy:0.5625\n",
      "Epoch_1 Batch_2000, Train Loss:0.03900418430566788, Accuracy:0.59375\n",
      "Epoch_1 Batch_2200, Train Loss:0.037210263311862946, Accuracy:0.5625\n",
      "Epoch_1 Batch_2400, Train Loss:0.03447109833359718, Accuracy:0.5625\n",
      "Epoch_1 Batch_2600, Train Loss:0.0430012121796608, Accuracy:0.34375\n",
      "Epoch_1 Batch_2800, Train Loss:0.040998637676239014, Accuracy:0.46875\n",
      "Epoch_1 Batch_3000, Train Loss:0.04246655851602554, Accuracy:0.46875\n",
      "Epoch_1 Batch_3200, Train Loss:0.03441359102725983, Accuracy:0.65625\n",
      "Epoch_1 Batch_3400, Train Loss:0.04251454025506973, Accuracy:0.5\n",
      "Epoch_1 Batch_3600, Train Loss:0.04668433219194412, Accuracy:0.375\n",
      "Epoch_1 Batch_3800, Train Loss:0.04124020040035248, Accuracy:0.5\n",
      ">>> Epoch_1, Train Loss:0.04018645705379508, Accuracy:0.5096917852108164\n",
      ">>> Epoch_1, Valid loss:0.0401222105917985, Accuracy:0.510957324106113 \n",
      "\n",
      "Epoch_2 Batch_0, Train Loss:0.04156331345438957, Accuracy:0.5\n",
      "Epoch_2 Batch_200, Train Loss:0.04039296507835388, Accuracy:0.53125\n",
      "Epoch_2 Batch_400, Train Loss:0.03605124354362488, Accuracy:0.5625\n",
      "Epoch_2 Batch_600, Train Loss:0.03609226644039154, Accuracy:0.5625\n",
      "Epoch_2 Batch_800, Train Loss:0.04672570154070854, Accuracy:0.375\n",
      "Epoch_2 Batch_1000, Train Loss:0.039132002741098404, Accuracy:0.59375\n",
      "Epoch_2 Batch_1200, Train Loss:0.045093342661857605, Accuracy:0.34375\n",
      "Epoch_2 Batch_1400, Train Loss:0.03854046389460564, Accuracy:0.5625\n",
      "Epoch_2 Batch_1600, Train Loss:0.04181404039263725, Accuracy:0.59375\n",
      "Epoch_2 Batch_1800, Train Loss:0.04862264171242714, Accuracy:0.34375\n",
      "Epoch_2 Batch_2000, Train Loss:0.03843311220407486, Accuracy:0.53125\n",
      "Epoch_2 Batch_2200, Train Loss:0.038204092532396317, Accuracy:0.5\n",
      "Epoch_2 Batch_2400, Train Loss:0.04633878916501999, Accuracy:0.34375\n",
      "Epoch_2 Batch_2600, Train Loss:0.036692071706056595, Accuracy:0.625\n",
      "Epoch_2 Batch_2800, Train Loss:0.040392205119132996, Accuracy:0.53125\n",
      "Epoch_2 Batch_3000, Train Loss:0.04189135879278183, Accuracy:0.40625\n",
      "Epoch_2 Batch_3200, Train Loss:0.040174487978219986, Accuracy:0.53125\n",
      "Epoch_2 Batch_3400, Train Loss:0.038954149931669235, Accuracy:0.53125\n",
      "Epoch_2 Batch_3600, Train Loss:0.037680502980947495, Accuracy:0.5\n",
      "Epoch_2 Batch_3800, Train Loss:0.035399992018938065, Accuracy:0.5625\n",
      ">>> Epoch_2, Train Loss:0.04017436895751043, Accuracy:0.5096917852108164\n",
      ">>> Epoch_2, Valid loss:0.04012318863542389, Accuracy:0.510957324106113 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "for e in range(epoch):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0\n",
    "    total_train_num = len(train_iter.dataset)\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        text = batch.Phrase.to(device)\n",
    "        label = batch.Sentiment.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out =model(text)\n",
    "        loss = criterion(out, label) \n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_accuracy = (torch.argmax(out, dim=1)==label).sum().item()\n",
    "        accuracy += batch_accuracy\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 200 == 0:\n",
    "            print('Epoch_{} Batch_{}, Train Loss:{}, Accuracy:{}'.format(e, i, batch_loss/len(label), batch_accuracy/len(label)))\n",
    "    print('>>> Epoch_{}, Train Loss:{}, Accuracy:{}'.format(e, total_loss/total_train_num, accuracy/total_train_num))\n",
    "    \n",
    "    # 每训练完一个epoch看下\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0\n",
    "    total_valid_num = len(valid_iter.dataset)\n",
    "    for batch in valid_iter:\n",
    "        text = batch.Phrase.to(device)\n",
    "        label = batch.Sentiment.to(device)\n",
    "        \n",
    "        out = model(text)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        total_loss += loss.item()\n",
    "        accuracy += (torch.argmax(out, dim=1)==label).sum().item()\n",
    "        \n",
    "    print('>>> Epoch_{}, Valid loss:{}, Accuracy:{} \\n'.format(e, total_loss/total_valid_num, accuracy/total_valid_num))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_learning]",
   "language": "python",
   "name": "conda-env-nlp_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
